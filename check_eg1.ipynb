{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch3d\n",
    "import pytorch3d.io\n",
    "import json\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch3d.vis.plotly_vis import AxisArgs, plot_batch_individually, plot_scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_path = './pose__01_11_gender_m_shape_624_texture_nongrey_male_0364_time_0'\n",
    "depth_path = './pose__01_11_gender_m_shape_624_texture_nongrey_male_0364_time_0_angle_3_x_0.06524504004767456_y_-0.4392347379791311_z_2.3664376918793324.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(depth_path, 'rb') as handle:\n",
    "    img = pickle.load(handle)\n",
    "\n",
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vertices, new_face_props, new_text_props = pytorch3d.io.load_obj(mesh_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code converts the depth image into a point cloud. The point cloud is stored as a numpy array.\n",
    "# It uses the camera intrinsics and the depth image to convert the depth image into a point cloud.\n",
    "# The point cloud is then stacked into a numpy array with the shape (num_points, 3).\n",
    "# The first dimension is the number of points in the point cloud.\n",
    "# The second dimension is the x, y, and z coordinates of the point cloud.\n",
    "\n",
    "img_size = (1080, 1920)\n",
    "height, width = img_size\n",
    "row = height\n",
    "col = width\n",
    "fov = 60\n",
    "aspect_ratio = width / height\n",
    "\n",
    "fy = height / (2. * np.tan(fov * np.pi / 360.))\n",
    "fx = width / (2. * np.tan(fov * np.pi / 360.))\n",
    "\n",
    "cx = width / 2.0\n",
    "cy = height / 2.0\n",
    "\n",
    "# Get the u and v pixel coordinates for each pixel in the image.\n",
    "v = np.array(list(np.ndindex((row, col)))).reshape(row, col, 2)[:, :, 0]\n",
    "u = np.array(list(np.ndindex((row, col)))).reshape(row, col, 2)[:, :, 1]\n",
    "\n",
    "img1 = img[:,:,0]\n",
    "\n",
    "# Calculate x, y, and z coordinates for each pixel in the image.\n",
    "X_ = (u - cx) / fx\n",
    "X_ = X_[img1 > -1]  # exclude infinity\n",
    "Y_ = (v - cy) / fy \n",
    "Y_ = Y_[img1 > -1]  # exclude infinity\n",
    "depth_ = img1[img1 > -1]  # exclude infinity\n",
    "\n",
    "# Calculate the x, y, and z coordinates of each pixel in the image.\n",
    "X = X_ * depth_ \n",
    "Y = Y_ * depth_ \n",
    "Z = depth_\n",
    "\n",
    "# Stack the x, y, and z coordinates into a point cloud.\n",
    "pcd_np = np.stack([X,Y,Z], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\")\n",
    "R = pytorch3d.transforms.euler_angles_to_matrix(torch.Tensor([90,180,0])/180 * np.pi, convention='XYZ').float().to(device)\n",
    "T = torch.tensor(np.array([0.06524504004767456, -0.4392347379791311, 2.3664376918793324])).float().to(device)\n",
    "cameras = pytorch3d.renderer.FoVPerspectiveCameras(\n",
    "                    R=R.T.unsqueeze(0), T=T.unsqueeze(0), fov=60, device=device, aspect_ratio=aspect_ratio\n",
    "                )\n",
    "xyz_unproj_world = cameras.unproject_points(torch.Tensor(pcd_np).to(device), world_coordinates=True)\n",
    "\n",
    "R_ = R.cpu().numpy()\n",
    "T_ = T.cpu().numpy()\n",
    "\n",
    "out =(pcd_np@(R_))+(T_)\n",
    "\n",
    "pcd_torch = torch.Tensor(pcd_np).to(device)\n",
    "try_output = torch.Tensor(out).to(device)\n",
    "xyz_unproj_world = torch.Tensor(xyz_unproj_world).to(device)\n",
    "\n",
    "if len(xyz_unproj_world.shape) < 3:\n",
    "    xyz_unproj_world = xyz_unproj_world.unsqueeze(0)\n",
    "    pcd_torch = pcd_torch.unsqueeze(0)\n",
    "    try_output = try_output.unsqueeze(0)\n",
    "\n",
    "rgbs = torch.ones_like(xyz_unproj_world).to(device)\n",
    "if len(rgbs.shape) < 3:\n",
    "    rgbs = rgbs.unsqueeze(0)\n",
    "        \n",
    "point_cloud_py3d_func = pytorch3d.structures.Pointclouds(points=xyz_unproj_world, features=rgbs*0.8).to(device)\n",
    "point_cloud_no_rt = pytorch3d.structures.Pointclouds(points=pcd_torch, features=rgbs*0.3).to(device)\n",
    "point_cloud_gt = pytorch3d.structures.Pointclouds(points=new_vertices.unsqueeze(0), features=torch.ones_like(new_vertices).unsqueeze(0)).to(device)\n",
    "point_cloud_our = pytorch3d.structures.Pointclouds(points=try_output, features=rgbs*0.6).to(device)\n",
    "\n",
    "fig2 = plot_scene({\n",
    "    \"figure\": {\n",
    "        \"pointcloud_pytorch3dfunc\": point_cloud_py3d_func,\n",
    "        \"pointcloud_gt\": point_cloud_gt,\n",
    "        \"pointcloud_withoutRT\": point_cloud_no_rt,\n",
    "        \"pointcloud_ours\": point_cloud_our,\n",
    "        \"Camera\": cameras,\n",
    "    }\n",
    "},\n",
    "    xaxis={\"backgroundcolor\":\"rgb(200, 200, 230)\"},\n",
    "    yaxis={\"backgroundcolor\":\"rgb(230, 200, 200)\"},\n",
    "    zaxis={\"backgroundcolor\":\"rgb(200, 230, 200)\"}, \n",
    "    axis_args=AxisArgs(showgrid=True))\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('pytorch3d_prohmr': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e3bd10790233985a87fc6ccf7c8a2386850426aabc26daf83e63c856b59fee2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
